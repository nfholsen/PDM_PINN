{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style='background-image: url(\"header.png\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
        "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
        "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
        "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Tutorial by Mondaic</div>\n",
        "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">For Salvus version 0.11.25</div>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to SalvusFlow's API\n",
        "\n",
        "Most user interaction with `SalvusFlow` should happen with the `salvus.flow.api` module. This tutorial presents a high-level introduction to the most important methods. For the full details please refer to `SalvusFlow`'s API documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Salvus on local or remote machines\n",
        "\n",
        "The API is used to submit Salvus jobs to run at either local or remote machines. These functions exist in synchronous/blocking and asynchronous/non-blocking variants. We'll explain what this means shortly. Furthermore there are variants that execute only a single simulation and variants than can run many simulations at once. The later are potentially a lot more efficient as they can use the native job array functionality of many job scheduling systems.\n",
        "\n",
        "* `salvus.flow.api.run_async()`: *Start/queue a single simulation and immediately return.*\n",
        "* `salvus.flow.api.run()`: *Start/queue a single simulation, wait for it to finish, copy all the outputs to the local machine, and delete all remote files.*\n",
        "* `salvus.flow.api.run_many_async()`: *Start/queue many simulation at once and immediately return.*\n",
        "* `salvus.flow.api.run_many()`: : *Start/queue many simulation at once, wait for them to finish, copy all the outputs to the local machine, and delete all remote files.*\n",
        "\n",
        "## Asynchronous vs. synchronos execution\n",
        "\n",
        "The synchronous variants are easy to understand: The functions run Salvus and wait until everything as completed before they return. This is most useful for small scale to medium scale simulations. The asynchronous variants submit/queue the jobs on the chosen site and then immediately return. They return `SalvusJob` or `SalvusJobArray` objects, respectively. These can be queries for the current status and once done they can also be used to get the output and many other things. This is useful for example for long-running/long-queuing jobs so one can do something else in the meanwhile.\n",
        "\n",
        "## Individual jobs vs. batch submission\n",
        "\n",
        "The `run_many...()` versions will execute multiple jobs at once. The major limitation here is that (due to how for example the Slurm job management system works) all jobs must run on the same number of cores and also must have the same wall time. Thus **the `run_many...()` functions are useful when running many similar jobs at once.** Similar jobs are jobs that hava a similar number of elements and time-steps. This is the case for most survey or inversion style studies where one for example simulates through the same domain but for many different sources.\n",
        "\n",
        "On sites which do not have a job queying systems (e.g. `local` and `ssh` sites) the jobs are internally run one after the other. On other sites they might potentially run in parallel, the details are up to the job scheduler.\n",
        "\n",
        "On system that support, e.g. slurm and others, the jobs will take advantage of their native job array support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examples\n",
        "\n",
        "### Setting up the simulations\n",
        "\n",
        "We will now set up all the required objects before we demonstrate how to use the various `run_...()` functions. These are very small simulations that can easily be run on a laptop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the api as well as the simple config and mesh objects.\n",
        "import os\n",
        "import shutil\n",
        "from salvus.flow import api\n",
        "from salvus.flow import simple_config as sc\n",
        "from salvus.mesh import simple_mesh as sm\n",
        "\n",
        "SALVUS_FLOW_SITE_NAME = os.environ.get(\"SITE_NAME\", \"local\")\n",
        "\n",
        "# A simple 2D homogeneous mesh.\n",
        "mesh = sm.CartesianHomogeneousIsotropicElastic2D(\n",
        "    vp=3000, vs=2000, rho=3000, x_max=1000, y_max=1000, max_frequency=5\n",
        ")\n",
        "\n",
        "# 17 equally spaced sources.\n",
        "sources = [\n",
        "    sc.source.cartesian.VectorPoint2D(\n",
        "        x=200,\n",
        "        y=300,\n",
        "        fx=100,\n",
        "        fy=200,\n",
        "        source_time_function=sc.stf.Ricker(center_frequency=5.0),\n",
        "    )\n",
        "    for x in list(range(100, 950, 50))\n",
        "]\n",
        "\n",
        "# We will now construct one simulation object per source.\n",
        "simulations = []\n",
        "for src in sources:\n",
        "    w = sc.simulation.Waveform(\n",
        "        mesh=mesh.create_mesh(), sources=src, receivers=[]\n",
        "    )\n",
        "    w.physics.wave_equation.end_time_in_seconds = 5.0\n",
        "    simulations.append(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running a single simulation synchronously\n",
        "\n",
        "With `salvus.flow.api.run()` SalvusFlow will run a simulation on the chosen machine, wait until it is done, retrieve the output (note the optional `overwrite` argument - it defaults to `False` in which case it fails if the folder already exists), and finally delete all remote files. This makes many things very convenient to use and it a very low friction way to run simulations and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api.run(\n",
        "    # We will only run a single simulation here.\n",
        "    input_file=simulations[0],\n",
        "    # The site to run on.\n",
        "    site_name=SALVUS_FLOW_SITE_NAME,\n",
        "    # Folder to which to copy the output to.\n",
        "    output_folder=\"output\",\n",
        "    overwrite=True,\n",
        "    wall_time_in_seconds=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running many simulation synchronously\n",
        "\n",
        "`salvus.flow.api.run_many()` will do the same as `salvus.flow.api.run()` but for many simulations at once. The output folder will afterwards contain a subfolder for each passed simulation object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api.run_many(\n",
        "    # Pass a list of simulation objects\n",
        "    input_files=simulations,\n",
        "    # The site to run on.\n",
        "    site_name=SALVUS_FLOW_SITE_NAME,\n",
        "    # Ranks and wall times have to be specified per job.\n",
        "    # Both are potentially optional (not all sites require)\n",
        "    # wall times, and if no ranks are given, it will always\n",
        "    # use the default number of ranks given when configuring the site.\n",
        "    ranks_per_job=2,\n",
        "    wall_time_in_seconds_per_job=60,\n",
        "    # Folder to which to copy the output to.\n",
        "    output_folder=\"output\",\n",
        "    # Overwrite the output folder if it already exists.\n",
        "    overwrite=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running a single simulation asynchronously\n",
        "\n",
        "The following example demonstrates how to run a single job asynchronously and how to work with the resulting `SalvusJob` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch a job in the background. Note that this function\n",
        "# will return immediately if there are no immediate errors.\n",
        "job = api.run_async(input_file=simulations[0], site_name=SALVUS_FLOW_SITE_NAME)\n",
        "\n",
        "# Query for the current status of the job with `.update_status()`.\n",
        "print(\"Current job status:\", job.update_status())\n",
        "\n",
        "# Do something else.\n",
        "print(\"Doing something else.\")\n",
        "\n",
        "# Wait for the job to finish. Blocks until the job is done.\n",
        "job.wait(\n",
        "    # Optional. Defaults to whatever is specified in\n",
        "    # the site configuration otherwise.\n",
        "    poll_interval_in_seconds=2.0,\n",
        "    # Optional. Wait at max this long before returning.\n",
        "    timeout_in_seconds=60.0,\n",
        ")\n",
        "\n",
        "# Query the status again.\n",
        "print(\"Current job status:\", job.update_status())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a dictionary with information about all remote output files.\n",
        "# These are not yet copied to the local machine.\n",
        "job.get_output_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the output files to the chosen folder. In this case\n",
        "# it is your responsibility to make sure that the folder does not yet exist.\n",
        "if os.path.exists(\"output_folder\"):\n",
        "    shutil.rmtree(\"output_folder\")\n",
        "job.copy_output(destination=\"output_folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next command deletes all files on the remote machine and removes it from the internal database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running many simulations asynchronously\n",
        "\n",
        "Same as the previous example but for many jobs this time around. We'll only use two simulations here to keep the output of some commands in check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_array = api.run_many_async(\n",
        "    # Only use the first two.\n",
        "    input_files=simulations[:2],\n",
        "    site_name=SALVUS_FLOW_SITE_NAME,\n",
        ")\n",
        "\n",
        "# Query for the current status of the jobs with `.update_status()`.\n",
        "print(\"Current status of jobs:\", job_array.update_status())\n",
        "\n",
        "# Do something else.\n",
        "print(\"Doing something else.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for the job to finish. Blocks until all jobs are done\n",
        "job_array.wait(verbosity=0)\n",
        "\n",
        "# Query the status again. Should all be finished now.\n",
        "print(\"Current status of jobs:\", job_array.update_status())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You still have access to each individual job.\n",
        "\n",
        "With the following call you will get a dictionary with information about all remote output files\n",
        "of the first job. These are not yet copied to the local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_array.jobs[0].get_output_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we want to actually copy the output files of all jobs to the chosen folder.  Note that it is the user's responsibility to make sure that the folder does not yet exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(\"output_folder\"):\n",
        "    shutil.rmtree(\"output_folder\")\n",
        "job_array.copy_output(destination=\"output_folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have all the files locally, we can safely delete the jobs on the remote machine and removes them from the internal database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_array.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Retrieve jobs and job arrays from the database\n",
        "\n",
        "The `SalvusJob` and `SalvusJobArray` objects can also be initialized from the database assuming the names and site names are known. This is useful for fully asynchronous workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Launch job.\n",
        "job = api.run_async(input_file=simulations[0], site_name=SALVUS_FLOW_SITE_NAME)\n",
        "# Retrieve again from DB.\n",
        "new_job = api.get_job(job_name=job.job_name, site_name=SALVUS_FLOW_SITE_NAME)\n",
        "# These two objects refer to the same job.\n",
        "assert job == new_job\n",
        "\n",
        "# The same logic holds for job arrays.\n",
        "job_array = api.run_many_async(\n",
        "    input_files=simulations[:2], site_name=SALVUS_FLOW_SITE_NAME\n",
        ")\n",
        "new_job_array = api.get_job_array(\n",
        "    job_array_name=job_array.job_array_name, site_name=SALVUS_FLOW_SITE_NAME\n",
        ")\n",
        "assert job_array == new_job_array"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_json": true,
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}